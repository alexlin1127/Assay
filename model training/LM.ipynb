{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\88697\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\88697\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# 下載 NLTK 資源\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# 初始化詞形還原器\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "# 將 NLTK 的詞性標籤轉換為 WordNet 的詞性標籤\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # 預設為名詞\n",
    "\n",
    "# 改進的詞形還原函數\n",
    "def lemmatize_words(words):\n",
    "    lemmatized = set()\n",
    "    for word in words:\n",
    "        word = word.lower()  # 轉小寫\n",
    "        tagged = nltk.pos_tag([word])\n",
    "        wordnet_pos = get_wordnet_pos(tagged[0][1])  # 獲得詞的詞性\n",
    "        lemmatized.add(lemmatizer.lemmatize(word, pos=wordnet_pos))  # 進行詞形還原\n",
    "    return lemmatized\n",
    "\n",
    "def load_sentiment_words(file_path):\n",
    "    df = pd.read_excel(file_path)\n",
    "    positive_words = set(df.iloc[:, 1].dropna().astype(str))  # 確保為字符串\n",
    "    negative_words = set(df.iloc[:, 0].dropna().astype(str))  # 確保為字符串\n",
    "    return positive_words, negative_words\n",
    "\n",
    "def determine_sentiment(sentence, positive_words, negative_words):\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    has_positive = any(word in positive_words for word in words)\n",
    "    has_negative = any(word in negative_words for word in words)\n",
    "\n",
    "    if has_negative:\n",
    "        return 'Negative'\n",
    "    elif has_positive and not has_negative:\n",
    "        return 'Positive'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# 載入詞庫\n",
    "positive_words, negative_words = load_sentiment_words('C:\\\\Users\\\\88697\\\\Downloads\\\\LM字典情緒詞庫.xlsx')  # 更改為您的文件路徑\n",
    "# 還原詞庫\n",
    "positive_words = lemmatize_words(positive_words)\n",
    "negative_words = lemmatize_words(negative_words)\n",
    "\n",
    "# 測試句子\n",
    "test_sentence = \"By applying voice intelligence to all external and internal communications, Dialpad is enabling organizations to sell more effectively, conduct more efficient meetings, personalize the customer experience, and make smarter business decisions automatically, in real-time, without installing new software.\"\n",
    "sentiment = determine_sentiment(test_sentence, positive_words, negative_words)\n",
    "print(f\"{sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取 CSV 文件，進行分析並保存結果\n",
    "def analyze_csv(input_file, output_file):\n",
    "    df = pd.read_csv(input_file)\n",
    "    df['LM label'] = df['sentence'].apply(lambda x: determine_sentiment(x, positive_words, negative_words))\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "# 執行分析（替換路徑）\n",
    "analyze_csv('C:\\\\Users\\\\88697\\\\Downloads\\\\AINews_label\\\\Total_data1.csv', 'C:\\\\Users\\\\88697\\\\Downloads\\\\AINews_label\\\\Total_data1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data: 671\n",
      "Number of mismatches: 275\n",
      "Accuracy of the model predictions: 0.59\n",
      "Precision: 0.45\n",
      "Recall: 0.47\n",
      "F1 Score: 0.44\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "# 讀取 CSV 檔案\n",
    "file_path = 'C:\\\\Users\\\\88697\\\\Downloads\\\\AINews_label\\\\Total_data1.csv'\n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "# 計算不匹配的次數\n",
    "mismatches = data['researcher2 label'] != data['LM label']\n",
    "mismatch_count = mismatches.sum()\n",
    "total_data = data['researcher2 label'] == data['researcher2 label']\n",
    "data_count = total_data.sum()\n",
    "\n",
    "# 計算準確率\n",
    "accuracy = accuracy_score(data['researcher2 label'], data['LM label'])\n",
    "# 計算精確率\n",
    "precision = precision_score(data['researcher2 label'], data['LM label'], average='macro')\n",
    "# 計算召回率\n",
    "recall = recall_score(data['researcher2 label'], data['LM label'], average='macro')\n",
    "# 計算 F1 分數\n",
    "f1 = f1_score(data['researcher2 label'], data['LM label'], average='macro')\n",
    "\n",
    "print(f\"Number of data: {data_count}\")\n",
    "print(f\"Number of mismatches: {mismatch_count}\")\n",
    "print(f\"Accuracy of the model predictions: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
